# Welcome to Data Structures

## General Notes

- The lasting goal of this text is to provide you with a solid and diverse mental toolbox of data structures so that, when you work on your own projects in the future, you will be able to recognize the appropriate data structure to use.

- Much like how a brain surgeon probably shouldn't use a butter knife to perform a surgery (even though it could technically work), you should always use the appropriate data structure, even if a less appropriate one would "technically work." In the following chapters, you will begin learning about the data structures covered in this text.

### Time Complexities
- There are three main notations to describe time complexity: Big-O ("Big-Oh"), Big-Ω ("Big-Omega"), and Big-ϴ ("Big-Theta"). Big-O notation provides an upper-bound on the number of operations performed, Big-Ω provides a lower-bound, and Big-ϴ provides both an upper-bound and a lower-bound. In other words, Big-ϴ implies both Big-O and Big-Ω. For our purposes, we will only consider Big-O notation because, in general, we only care about upper-bounds on our algorithms.
- For example, say we have a list of n numbers and, for each number, we want to print out the number and its opposite. Assuming the "print" operation is a single operation and the "opposite" operation is also a single operation, we will be performing 3 operations for each number (print itself, return the opposite of it, and print the opposite).
- If we have n elements and we will be performing exactly 3 operations on each element, we will be performing exactly 3n operations. Therefore, we would say that this algorithm is O(n), because there exists some constant (3) for which the number of operations performed by our algorithm is always bounded by the function g(n) = n.
Note that, although this algorithm is O(n), it is also technically O(n²) and any other larger function of n, because technically, these are all functions that are upper-bounds on the function f(n) = 3n. However, when we describe algorithms, we always want to describe them using the tightest possible upper-bound.
- Here's another example: say I have an algorithm that, given a list of n students, prints out 5 header lines (independent of n) and then prints the n students' names and grades on separate lines. This algorithm will perform 2n + 5 operations total: 2 operations per student (print name, print grade) and 5 operations independent of the number of students (print 5 header lines). The time complexity of this algorithm in Big-O notation would be O(2n + 5), which we would simplify to O(n) because we drop the constant (2n becomes n) and drop all lower terms (5 < n as n becomes large).

- "Good" Time Complexities
    - "Constant Time" = O(1)
    - "Logarithmic Time" = "Scales/Increases Logarithmically" = O(log n)
    - "Polynomial Time" = "Scales/Increases Polynomially" = O(nk) (for any constant k)

- "Bad" Time Complexities
    - "Exponential Time" = "Scales/Increases Exponentially" = O(kⁿ) (for any constant k)
    - "Factorial Time" = O(n!)

[Advanced Data Structures: Common Big-O Time Complexities](https://www.youtube.com/watch?v=InLKb4ama0o&t=36s)

### Data Structures

- A data structure, as implied by the name, is a particular structured way of storing data in a computer so that it can be used efficiently.
- When dealing with data storage, in addition to worrying about time complexity, we will also be worrying about space complexity,

[Advanced Data Structures: Space Complexity](https://www.youtube.com/watch?v=E3HJgHA2XZc)
